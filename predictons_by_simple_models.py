# -*- coding: utf-8 -*-
"""Predictons by simple models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VNLhk-ynb9_cZ2TvpBOyMgHBuPTZzBnD
"""

!pip install yfinance --quiet

import os
import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime
from typing import Dict, Any, Iterable

tickers = ["AAPL"]
start = "2015-01-01"
end = "2025-10-30"

out_dir = "/content/data"
os.makedirs(out_dir, exist_ok=True)

all_dfs = []
for t in tickers:
    tick = t
    print(f"Downloading {tick} ...")
    df = yf.download(tick, start=start, end=end, progress=False, auto_adjust=False)

    #  столбцы — MultiIndex, пытаемся раскрыть нужный уровень
    if isinstance(df.columns, pd.MultiIndex):
        #  верхний уровень = тикер, нижний = ['Open','High',...]
        if df.columns.nlevels == 2:
            lvl0 = df.columns.get_level_values(0)
            lvl1 = df.columns.get_level_values(1)
            expected = ["Open", "High", "Low", "Close", "Volume", "Adj Close"]
            if set(expected).issubset(set(lvl1)):
                df.columns = lvl1
            elif set(expected).issubset(set(lvl0)):
                df.columns = lvl0
            else:
                df.columns = ['_'.join(map(str, c)).strip() for c in df.columns.values]
        else:
            df.columns = ['_'.join(map(str, c)).strip() for c in df.columns.values]

    df = df.reset_index().rename(columns={
        "Date": "date",
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "Adj Close": "adj_close",
        "Volume": "volume"
    })

    # Если adj_close отсутствует — копируем close
    if "adj_close" not in df.columns:
        df["adj_close"] = df["close"]

    df["ticker"] = tick
    df = df.sort_values("date").reset_index(drop=True)

    df["prev_close"] = df["adj_close"].shift(1)
    df["return"] = df["adj_close"].pct_change()
    df["log_return"] = np.log(df["adj_close"] / df["prev_close"])
    df["hl_spread"] = df["high"] - df["low"]
    df["oc_spread"] = (df["close"] - df["open"]) / df["open"]
    df["typical_price"] = (df["high"] + df["low"] + df["close"]) / 3
    df["dayofweek"] = df["date"].dt.dayofweek
    df["month"] = df["date"].dt.month
    df["year"] = df["date"].dt.year
    df["is_month_start"] = df["date"].dt.is_month_start.astype(int)
    df["next_return"] = df["close"].shift(-1) / df["close"] - 1

    keep_cols = [
        "date", "ticker", "open", "high", "low", "close", "adj_close", "volume",
        "prev_close", "return", "log_return", "hl_spread", "oc_spread",
        "typical_price", "dayofweek", "month", "year", "is_month_start",
        "next_return"
    ]
    keep_existing = [c for c in keep_cols if c in df.columns]
    df = df[keep_existing]
    df = df.dropna(subset=["open", "high", "low", "close"], how="any")

    all_dfs.append(df)

# Конкатенируем только если есть данные
if len(all_dfs) == 0:
    raise RuntimeError("No valid dataframes collected (all_dfs is empty). Check the diagnostic messages above.")
prices = pd.concat(all_dfs, ignore_index=True)
prices = prices.sort_values(["ticker", "date"]).reset_index(drop=True)

prices.to_csv("/content/data/prices.csv", index=False)

# Вывод какой-то статистики и превью данных

pd.set_option("display.max_rows", 200)
pd.set_option("display.max_columns", 50)
pd.set_option("display.width", 140)

print("=== preview (first 20 rows) ===")
display(prices.head(20))

print("\n=== info ===")
print(prices.info())

print("\n=== basic describe (numeric cols) ===")
display(prices.describe().T)

print("\n=== nulls per column ===")
print(prices.isnull().sum())

print("\n=== dtypes ===")
print(prices.dtypes)

# Ensure date is datetime
if not pd.api.types.is_datetime64_any_dtype(prices['date']):
    print("\nConverting prices['date'] to datetime...")
    prices['date'] = pd.to_datetime(prices['date'])
    print("Converted.")

print("\n=== summary by ticker: date range, rows, missing values ===")
summary = prices.groupby("ticker").agg(
    date_min = ("date","min"),
    date_max = ("date","max"),
    rows = ("date","count"),
    n_null_adj_close = ("adj_close", lambda s: s.isnull().sum()),
    n_null_volume = ("volume", lambda s: s.isnull().sum()),
)
display(summary.sort_values(["rows"], ascending=False))

print("\n=== duplicate dates per ticker (should be 0) ===")
dups = prices.groupby("ticker").apply(lambda g: g['date'].duplicated().sum()).rename("n_duplicate_dates")
display(dups.sort_values(ascending=False).head(20))

print("\n=== sample rows for each ticker (last 3 rows) ===")
for t, g in prices.groupby("ticker"):
    print(f"\n--- {t} ({len(g)} rows) last 3 rows ---")
    display(g.sort_values("date").tail(3))

print("\n=== quick consistency checks ===")
# prev_close vs adj_close.shift(1) per ticker
def check_prev_close_consistency(g):
    # compare prev_close column with adj_close shifted within the same ticker
    if 'prev_close' not in g.columns:
        return pd.Series({"prev_close_missing": True})
    s = (g['prev_close'] != g['adj_close'].shift(1)) & ~(g['prev_close'].isnull() & g['adj_close'].shift(1).isnull())
    return pd.Series({
        "n_prev_close_mismatch": int(s.sum()),
        "pct_prev_close_mismatch": float(s.sum() / max(1, len(g)))
    })

consistency = prices.groupby("ticker").apply(check_prev_close_consistency)
display(consistency.sort_values("n_prev_close_mismatch", ascending=False).head(20))

print("\n=== end checks ===")

"""# Шаг 2
генерация фичей
"""

def rsi(series: pd.Series, window: int = 14) -> pd.Series:
  delta=series.diff()
  gain=delta.clip(lower=0.0)
  loss=-delta.clip(upper=0.0)
  avg_gain=gain.ewm(alpha=1.0/window,adjust=False).mean()
  avg_loss = loss.ewm(alpha=1.0/window, adjust=False).mean()
  rs= avg_gain / (avg_loss.replace(0, np.nan))
  rsi = 100 - (100 / (1 + rs))
  rsi = rsi.fillna(100.0 * (avg_loss == 0).astype(float))
  return rsi

LAGS = (1, 2, 3)
MA_WINDOWS = (5, 20)
RSI_WINDOW = 14
VOL_WINDOW = 10
MOMENTUM_WINDOW = 5

df = pd.read_csv("/content/data/prices.csv", parse_dates=["date"])

price_col = "adj_close"

df=df.sort_values(["date"]).reset_index(drop=True)
g=df.groupby("ticker",group_keys=False)
for lag in LAGS:
  df[f"r_lag{lag}"]=g["return"].shift(lag)

# moving averages
for w in MA_WINDOWS:
    df[f"ma{w}"] = g[price_col].transform(lambda s, ww=w: s.rolling(window=ww, min_periods=1).mean())

df[f"rsi{RSI_WINDOW}"] = g[price_col].transform(lambda s: rsi(s, window=RSI_WINDOW))
df[f"vol_{VOL_WINDOW}"] = g["return"].transform(lambda s: s.rolling(window=VOL_WINDOW, min_periods=1).std())
df[f"momentum_{MOMENTUM_WINDOW}"] = g[price_col].transform(lambda s: s / s.shift(MOMENTUM_WINDOW) - 1)
#volume fetures
df["vol_change"] = g["volume"].pct_change()
df["vol_roll_mean_20"] = g["volume"].transform(lambda s: s.rolling(window=20, min_periods=1).mean())
df["vol_roll_std_20"] = g["volume"].transform(lambda s: s.rolling(window=20, min_periods=1).std())

df["target"] = np.nan

df.to_csv("/content/data/features.csv", index=False)

display(df.head(12))

"""# Шаг 3
делаем таргет
"""

df.describe()

# target: 1 если next_return > 0 иначе 0
df["target"] = (df["next_return"] > 0).astype(int)

df_clean = df.dropna(subset=["next_return"]).reset_index(drop=True)
df_clean.to_csv("/content/data/dataset.csv", index=False)

"""# шаг 4
сплит данных
"""

TRAIN_END = "2020-12-31"   # включительно в train
VAL_END   = "2022-12-31"   # включительно в val (val диапазон 2021-01-01 .. 2022-12-31)
TEST_START = "2023-01-01"  # тест с 2023-01-01 и дальше
train_end_dt = pd.to_datetime(TRAIN_END)
val_end_dt = pd.to_datetime(VAL_END)
test_start_dt = pd.to_datetime(TEST_START)
np.random.seed(42)

train_df=df_clean.loc[df_clean["date"]<=train_end_dt].reset_index(drop=True)
val_df=df_clean.loc[(df_clean['date'] > train_end_dt) & (df_clean['date'] <= val_end_dt)].reset_index(drop=True)
test_df=df_clean.loc[df_clean["date"]>=test_start_dt].reset_index(drop=True)

train_path = os.path.join("/content/data", "train.csv")
val_path = os.path.join("/content/data", "val.csv")
test_path = os.path.join("/content/data", "test.csv")
train_df.to_csv(train_path, index=False)
val_df.to_csv(val_path, index=False)
test_df.to_csv(test_path, index=False)

num_features=[c for c in df_clean.select_dtypes(include=[np.number]).columns.tolist() if c not in {"target", "next_return"}]
num_features

feat_file = os.path.join("/content/data", "recommended_features.txt")
with open(feat_file, "w", encoding="utf-8") as f:
    for c in num_features:
        f.write(c + "\n")

"""# Шаг 5
обучаем модели
"""

!pip install lightgbm xgboost joblib --quiet

!pip install optuna

import optuna
from pathlib import Path
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score
from sklearn.utils import check_random_state
import joblib
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier
os.makedirs("/content/models", exist_ok=True)

train=train_df.copy()
val =val_df.copy()
test= test_df.copy()
features=[c for c in num_features if c not in {"date", "ticker", "open", "high", "low", "close", "adj_close", "prev_close", "typical_price", 'ma20','ma5'}] # лучше убрать явные признаки , так как модель будет сильно обращать внимание именно на цену , что будет мешать учится
print("Number of features:", len(features))

print(f"Доля единиц в train в столбце target: {train["target"].mean()}")

"""обучающая выборка не предвзята"""

X_train, y_train = train[features], train["target"]
X_val, y_val     = val[features], val["target"]
X_test, y_test   = test[features], test["target"]
X_full = pd.concat([X_train, X_val], axis=0).reset_index(drop=True)
y_full = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)
tscv = TimeSeriesSplit(n_splits=5)

"""**LightGBM**"""

def objective_lgb_ts(trial):
    param = {
        "objective": "binary",
        "metric": "auc",
        "verbosity": -1,
        "boosting_type": "gbdt",
        "random_state":42 ,
        "num_leaves": trial.suggest_int("num_leaves", 16, 256),
        "min_child_samples": trial.suggest_int("min_child_samples", 5, 200),
        "subsample": trial.suggest_float("subsample", 0.4, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.4, 1.0),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-8, 10.0),
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-8, 10.0),
        "learning_rate": trial.suggest_loguniform("learning_rate", 1e-3, 0.5),
    }
    aucs = []
    best_iters = []
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_full)):
        X_tr, X_va = X_full.iloc[train_idx], X_full.iloc[val_idx]
        y_tr, y_va = y_full.iloc[train_idx], y_full.iloc[val_idx]

        model = lgb.LGBMClassifier(**param, n_estimators=2000)
        callbacks = [
        lgb.early_stopping(stopping_rounds=10),
        lgb.log_evaluation(period=0)  ]

        model.fit(
            X_tr, y_tr,
            eval_set=[(X_va, y_va)],
            eval_metric="auc",
            callbacks=callbacks,
        )

        y_pred = model.predict_proba(X_va)[:, 1]
        aucs.append(roc_auc_score(y_va, y_pred))
        try:
            best_iters.append(int(model.best_iteration_))
        except Exception:
            pass

    mean_auc = float(np.mean(aucs))
    trial.set_user_attr("best_it_median", int(np.median(best_iters)))
    return mean_auc

study_lgb = optuna.create_study(direction="maximize", sampler=optuna.samplers.TPESampler(seed=42))
study_lgb.optimize(objective_lgb_ts, n_trials=200, show_progress_bar=True)

print("LGBM best params:", study_lgb.best_trial.params)
print("LGBM best  AUC:", study_lgb.best_value)
best_it_lgb = study_lgb.best_trial.user_attrs.get("best_it_median", None)
print("LGBM median best_it per fold:", best_it_lgb)

# тренируем на лучших параметрах
best_params_lgb = study_lgb.best_trial.params.copy()
final_n_estimators_lgb = int(best_it_lgb) if best_it_lgb is not None else 200
final_lgb = lgb.LGBMClassifier(**best_params_lgb, n_estimators=final_n_estimators_lgb)
final_lgb.fit(X_full, y_full)

lgb_probs_test = final_lgb.predict_proba(X_test)[:, 1]
lgb_auc_test = roc_auc_score(y_test, lgb_probs_test)
print(f"Final LGBM test AUC: {lgb_auc_test:.4f}")
joblib.dump(final_lgb, "models/lgbm_ts_cv_best.pkl")

"""Модель практически не обучилась , 0.47 - это хуже рандома"""

def objective_xgb_ts(trial):
    param = {
        "n_estimators": 2000,
        "booster": "gbtree",
        "use_label_encoder": False,
        "eval_metric": "auc",
        "random_state": 42,
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.2, log=True),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "subsample": trial.suggest_float("subsample", 0.4, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.4, 1.0),
        "gamma": trial.suggest_float("gamma", 0.0, 5.0),
        "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 10.0, log=True),
        "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 10.0, log=True),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 20),
        "n_jobs": -1,
    }

    aucs = []
    best_iters = []
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_full)):
        X_tr, X_va = X_full.iloc[train_idx], X_full.iloc[val_idx]
        y_tr, y_va = y_full.iloc[train_idx], y_full.iloc[val_idx]

        model = XGBClassifier(**param)

        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)

        y_pred = model.predict_proba(X_va)[:, 1]
        aucs.append(roc_auc_score(y_va, y_pred))
        try:
            best_iters.append(int(model.get_booster().best_iteration))
        except Exception:
            pass

    mean_auc = float(np.mean(aucs))
    if len(best_iters) > 0:
        trial.set_user_attr("best_it_median", int(np.median(best_iters)))
    return mean_auc




study_xgb = optuna.create_study(direction="maximize", sampler=optuna.samplers.TPESampler(seed=42))
study_xgb.optimize(objective_xgb_ts, n_trials=30, show_progress_bar=True)

print("XGB best params:", study_xgb.best_trial.params)
print("XGB best CV AUC:", study_xgb.best_value)
best_it_xgb = study_xgb.best_trial.user_attrs.get("best_it_median", None)
print("XGB median best_it per fold:", best_it_xgb)

# тренируем на лучшиз параметрах
best_params_xgb = study_xgb.best_trial.params.copy()
final_n_estimators_xgb = int(best_it_xgb) if best_it_xgb is not None else 200
final_xgb = XGBClassifier(
    n_estimators=final_n_estimators_xgb,
    learning_rate=best_params_xgb.get("learning_rate", 0.02),
    max_depth=int(best_params_xgb.get("max_depth", 6)),
    subsample=best_params_xgb.get("subsample", 0.8),
    colsample_bytree=best_params_xgb.get("colsample_bytree", 0.8),
    gamma=best_params_xgb.get("gamma", 0.0),
    reg_alpha=best_params_xgb.get("reg_alpha", 1e-8),
    reg_lambda=best_params_xgb.get("reg_lambda", 1e-8),
    min_child_weight=int(best_params_xgb.get("min_child_weight", 1)),
    use_label_encoder=False,
    eval_metric="auc",
    random_state=42,
    n_jobs=-1
)
final_xgb.fit(X_full, y_full)

xgb_probs_test = final_xgb.predict_proba(X_test)[:, 1]
xgb_auc_test = roc_auc_score(y_test, xgb_probs_test)
print(f"Final XGB test AUC: {xgb_auc_test:.4f}")
joblib.dump(final_xgb, "models/xgb_ts_cv_best.pkl")

"""XGBoost тоже плохо работает . Видимо предсказвать цену по volume  и цене предыдущих дней попросту невозможно"""

print(f"LGBM CV AUC (best): {study_lgb.best_value:.4f} -> test AUC final: {lgb_auc_test:.4f}")
print(f"XGB  CV AUC (best): {study_xgb.best_value:.4f} -> test AUC final: {xgb_auc_test:.4f}")

"""Видимо предсказвать цену по volume и цене предыдущих дней попросту невозможно"""

